---
title: "Final Project Analysis"
author: "Joy Y Kim"
date: "2025-05-09"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: false
---
```{r setup , include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

## R libraries used in this analysis

library(tidyverse)
library(knitr)
library(leaps)
library(caret)
library(car)
library(lmtest)
```

# Introduction

## Dataset Description
This is a statistical analysis of the dataset titled “Student Habits vs Academic Performance: A Simulated Study”, as found on Kaggle (https://www.kaggle.com/datasets/jayaantanaath/student-habits-vs-academic-performance?resource=download). The dataset features 1,000 student records, where each row represents an individual student. The data captures the following variables: 

* age: age in years
* gender: female, male, or other
* study_hours_per_day: daily study time in hours
* social_media_hours: daily social media usage in hours
* netflix_hours: daily Netflix usage in hours
* part_time_job: if the student holds a part time job (Yes or No)
* attendance_percentage: percentage student attended school
* sleep_hours: daily sleep time in hours
* diet_quality: diet quality (Poor, Fair, or Good)
* exercise_frequency: frequency of exercise per week (0-6 days per week)
* parental_education_level: highest level of education student's parents have attained (None, High School, Bachelor, Master)
* internet_quality: quality of student's Internet (Poor, Average, Good)
* mental_health_rating: rating of student's mental health on a scale of 1-10
* extracurricular_participation: if the student participates in extracurricular activities (Yes or No)
* exam_score: final exam score on a scale of 0-100

## Problem Statement
This analysis aims to examine the extent to which demographic, lifestyle, academic, and environmental factors jointly relate to final exam scores among students.

# Methodology

The overall methodology used for this statistical analysis is linear regression. The following steps are outlined as followd:

1. Data cleaning and preprocessing

2. Summary statistics of variables and visualization of distributions and relationships

3. Variable selection and hypothesis testing

4. Model performance assessment

5. Model validation 

6. Verification of linear regression assumptions

7. Handling of assumption violations

8. Feature impact analysis and interpretation

## Exploratory Data Analysis

```{r}
# Load the dataset
studentData <- read.csv("Data/student_habits_performance.csv")
names(studentData)
dim(studentData)
```

## Data Cleaning and Preprocessing Steps
```{r}
str(studentData)

# Convert categorical variables to factors
studentData$gender <- as.factor(studentData$gender)
studentData$part_time_job <- as.factor(studentData$part_time_job)
studentData$diet_quality <- as.factor(studentData$diet_quality)
studentData$parental_education_level <- as.factor(studentData$parental_education_level)
studentData$internet_quality <- as.factor(studentData$internet_quality)
studentData$extracurricular_participation <- as.factor(studentData$extracurricular_participation)

#remove first column
studentData <- studentData[ , -1]
```

## Identification of Missing Values and Outliers
```{r}
# Identification of missing values
colSums(is.na(studentData)) #no missing values

# Identification of outliers
numeric_vars <- studentData[sapply(studentData, is.numeric)]

# Boxplots for all numeric variables
boxplot(numeric_vars, main = "Boxplots for All Numeric Variables", las = 2)

```

## Summary Statistics of Variables
```{r}
summary(studentData)
```

## Visualization of Distributions and Relationships
```{r}
#histograms of numerical variables

hist(studentData$exam_score, main = "Distribution of Exam Scores", xlab = "Score", col = "lightblue")

hist(studentData$study_hours_per_day, main = "Distribution of Study Hours", xlab = "Hours", col = "lightblue")

hist(studentData$social_media_hours, main = "Distribution of Social Media Hours", xlab = "Hours", col = "lightblue")

hist(studentData$netflix_hours, main = "Distribution of Netflix Hours", xlab = "Hours", col = "lightblue")

hist(studentData$attendance_percentage, main = "Distribution of Attendance Percentage", xlab = "Percentage", col = "lightblue")

hist(studentData$sleep_hours, main = "Distribution of Sleep Hours", xlab = "Hours", col = "lightblue")

hist(studentData$exercise_frequency, main = "Distribution of Exercise Frequnecy", xlab = "Frequency", col = "lightblue")

hist(studentData$mental_health_rating, main = "Distribution of Mental Health Rating", xlab = "Rating", col = "lightblue")
```

# Analysis

## Variable Selection & Hypothesis Testing

I will be implementing three different variable selection techniques: forward selection, backward selection, and the branch and bound method. The branch and bound method is feasible as the number of predictors is 14. I did, however, want to compare this method to the more computationally efficient stepwise selection methods as well.

```{r}
# Variable selection technique 1: Forward selection

## smallest model to consider
intOnly <- lm(exam_score~ 1, data = studentData)

## largest model to consider
fullmod <- lm(exam_score ~ ., data = studentData)

out_forward_bic <- step(object = intOnly, direction = "forward",
                        scope = formula(fullmod), trace = T, k = log(nrow(studentData)))

# Model summary and assess model performance with metrics
summary(out_forward_bic)
```

```{r}
# Variable selection technique 2: Backward selection

out_backward_bic <- step(object = fullmod, direction = "backward",
                        scope = formula(fullmod), trace = T, k = log(nrow(studentData)))

# Model summary and assess model performance with metrics
summary(out_backward_bic)

```


```{r}
# Variable selection technique 3: Branch and bound selection

library(leaps)

X <- model.matrix(exam_score ~ ., data = studentData)[, -1]
Y <- studentData$exam_score
n <- nrow(X)
p <- ncol(X)

out_leaps <- regsubsets(x = X, y = Y, nvmax = p, method = "exhaustive")
sout <- summary(out_leaps)

# Calculate BIC
bic <- -n/2 * log(sout$rss / n) - log(n)/2 * (1:p + 2)

# Best model selected by BIC
which.max(bic)

# Model
selected_variables <- colnames(X)[sout$which[which.max(bic), -1]]
print(selected_variables)
```

## Assessing Model Performance with Metrics

For model selection purposes, BIC will be used as the primary metric to assess model performance as the goal of this analysis is to explain which factors matter most on student final exam scores, not just maximize predictive accuracy. This will aid with avoiding overfitting as BIC penalizes unnecessary predictors more harshly than AIC. In addition, since the sample size is large, BIC tends to outperforms AIC for model selection consistency.

The model selected using forward selection is: 
```{r}
forward_mod <- lm(exam_score ~ study_hours_per_day + mental_health_rating + social_media_hours + 
                    exercise_frequency + sleep_hours + netflix_hours + attendance_percentage, data=studentData)
```

The model selected using backward selection is: exam_score ~ study_hours_per_day + social_media_hours + netflix_hours + attendance_percentage + sleep_hours + exercise_frequency + mental_health_rating
```{r}
backward_mod <- lm(exam_score ~ study_hours_per_day + mental_health_rating + social_media_hours + 
                     exercise_frequency + sleep_hours + netflix_hours + attendance_percentage, data=studentData)
```

The two models are the same, hence the BIC of each model is the same (BIC of both models = 3394.18).

The model selected using the brand and bound method and BIC contains the same covariates as the the models selected by the forward and backward selection methods. The final model is: 
```{r}
final_mod <- lm(exam_score ~ study_hours_per_day + social_media_hours + 
    netflix_hours + attendance_percentage + sleep_hours + exercise_frequency + 
    mental_health_rating, data = studentData)

summary(final_mod)
```



## Validating Model using Appropriate Cross-Validation Techniques

```{r}
#K-fold cross validation

library(caret)

# Setting up 5-fold cross-validation
control <- trainControl(method = "cv", number = 5)

# Fitting selected model with cross-validation
cv_model <- train(
  exam_score ~ study_hours_per_day + social_media_hours + 
    netflix_hours + attendance_percentage + sleep_hours + 
    exercise_frequency + mental_health_rating,
  data = studentData,
  method = "lm",
  trControl = control
)

# Results
cv_model
```

The Root Mean Squared Error (RSME) is 5.37, which means on average, the model's predictions deviate from the actua exam scores by around 5.35 points, which is relatively small compared to a 100-point scale. The mean absolute error (MAE) is 4.24 points, which is also quite low. The R-squared is 0.90, which means the model explains 90% of the variance in exam scores, which is strong.

## Regression Assumptions Verification

### Linearity Assessment - Fitted vs Residuals Plot

```{r}
plot(final_mod$fitted, studentData$exam_score,
     main = "Actual vs Fitted Plot",
     xlab = "Fitted Values",
     ylab = "Actual Values")
abline(a = 0, b = 1, col = "red", lty = 2)
```

### Normality of Residuals

```{r}
library(car)
qqPlot(final_mod, main = "Q-Q Plot with Confidence Band")

# Shapiro-Wilk Test
shapiro.test(residuals(final_mod))
```

### Homoscedasticity (Constant Variance of Residuals)

```{r}
plot(final_mod$fitted.values, final_mod$residuals,
     main = "Residuals vs Fitted Values Plot",
     xlab = "Fitted Values",
     ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

# Breusch-Pagan Test
library(lmtest)
bptest(final_mod)
```

### Independence of Observations

For this assumption to be met, each student's data should be unrelated to another student's. Since the data was not collected by myself nor is the data collection process transparent, it cannot be determined if all students were sampled randomly and individually or if students from different classrooms/schools are nested or not. It is likely that there are repeated measures per student, however. As such, it cannot be stated with 100% certainty that the observations in this dataset are independent.

#### Multicollinearity Assessment

```{r}
library(car)
vif(final_mod)
```

### Initial Overall Assessment of (Untransformed) Linear Model Assumptions

Linearity assumption: The fitted vs actual values plot overall shows a pretty linear relationship, thus satisfying this assumption.

Normality of residuals assumption: The QQ plot shows some points at the bottom end of the plot outside of the confidence band. In addition, a Shapiro-Wilk test of the residuals results in a p-value of 0.003196, leading to rejection of the null hypothesis of normality of the residuals at an alpha significance level of 0.05, in favor of the alternative hypothesis that the residuals are not normally distributed. This assumption is thus violated and will be addressed (see section below).

Homoscedasticity assumption: The residuals vs fitted plot depicts that the spread of residuals narrows at higher fitted values, showing a visible pattern (particularly towards the right side). This suggests non-constant variance. However, the Breusch-Pagan Test resulted in a p-value of 0.5697. As such, we fail to reject the null hypothesis of homoscedasticity at the alpha = 0.05 significance level. Overall, based on the formal Breusch-Pagan Test, the homoscedasticity assumption is determined to be met.

Independence of observations assumption: As stated previously, it cannot be determined with 100% certainty that the observations in this dataset are independent due to lack of information on how the data was collected. For the purposes of this analysis and the restrictions placed on available data, it will be assumed that the independence of observations assumption is met.

Multicollinearity assessment: The variance inflation factor for each predictor in the model is around 1, suggesting that there is no multicollinearity between predictors. 

## Assumption Violation Handling

### Addressing Violation of the Normality of Residuals Assumption

I initially tried to transform the dependent variable, but this did not help with violation of the normality of residuals assumption. I then tried to transform covariates in order of ones that were most skewed and/or had many outliers: attendance percentage and Netflix hours. Finally, I log transformed the mental_health_rating variable, which is not normally distributed according to the Shapiro-Wilk test (p < 2.2e-16).

```{r}
# Documenting Approach to Violation

#Transformation of covariate
log_final_mod <- lm(exam_score ~ study_hours_per_day + social_media_hours + 
    netflix_hours + attendance_percentage + sleep_hours + exercise_frequency + 
  log(mental_health_rating), data = studentData)

#Re-assessing normality of residuals assumption
qqPlot(log_final_mod, main = "Q-Q Plot with Confidence Band")

# Shapiro-Wilk Test
shapiro.test(residuals(log_final_mod))
```
```{r}
# Re-assessing all other assumptions / comparing models after correction

# Linearity assumption
plot(log_final_mod$fitted, studentData$exam_score,
     main = "Actual vs Fitted Plot",
     xlab = "Fitted Values",
     ylab = "Actual Values")
abline(a = 0, b = 1, col = "red", lty = 2)

# Homoscedasticity (constant variance of residuals)
plot(log_final_mod$fitted.values, log_final_mod$residuals,
     main = "Residuals vs Fitted Values Plot",
     xlab = "Fitted Values",
     ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

# Breusch-Pagan Test
bptest(log_final_mod)

# Multicollinearity
vif(log_final_mod)
```

### Overall Assessment of Transformed Linear Model Assumptions

Linearity assumption: The fitted vs actual values plot overall continues to show an overall fairly linear relationship, thus satisfying this assumption.

Normality of residuals assumption: The QQ plot shows nearly all of the points within the confidence band. In addition, a Shapiro-Wilk test of the residuals results in a p-value of 0.2025, leading to failure of rejecting the null hypothesis of normality at an alpha significance level of 0.05. This assumption is now met.

Homoscedasticity assumption: The residuals vs fitted plot continues to show that the spread of residuals narrows at higher fitted values, with the same visible pattern particularly towards the right side. This again potentially suggests non-constant variance. However, the Breusch-Pagan Test resulted in a p-value of 0.8019. As such, we fail to reject the null hypothesis of homoscedasticity at the alpha = 0.05 significance level. Overall, based on the formal Breusch-Pagan Test, the homoscedasticity assumption is met.

Independence of observations assumption: See previous answer as this continues to be the same.

Multicollinearity assessment: The variance inflation factor for each predictor in the model, with the newly transformed covariate, continues to be around 1, suggesting no multicollinearity between predictors. 

```{r}
# Comparing model performance after correction

BIC(final_mod) # initial, untransformed model
BIC(log_final_mod) #transformed model

# Note: the above method for calculating BIC is slightly different than the method used in step-wise selection; as long as BIC values are derived from the same method when comparing models, the practical results should be the same

#summary of transformed model
summary(log_final_mod)
```
```{r}
#K-fold cross validation of transformed model

# Setting up 5-fold cross-validation
control2 <- trainControl(method = "cv", number = 5)

# Fitting selected model with cross-validation
cv_model2 <- train(
  exam_score ~ study_hours_per_day + social_media_hours + 
    netflix_hours + attendance_percentage + sleep_hours + 
    exercise_frequency + log(mental_health_rating),
  data = studentData,
  method = "lm",
  trControl = control2
)

# Results
cv_model2
```


# Results 

## Untransformed vs Transformed Model Comparison

The initial model that was selected using the branch and bound method and assessed using BIC, yielded the best predictive performance. However, this model violated the assumption of normality of residuals. Attempts to transform the dependent variable did not resolve this. Instead, transforming selected covariates (log(mental_health_rating)) improved the residual normality, satisfying this linear model assumption. However, this came at the cost of a significantly worse BIC (~115 points higher) and slightly worse cross-validation error (untransformed model RSME = 5.35, transformed model RSME = 5.65).


## Feature Impact Analysis

### Quantifying and Interpreting the Impact of Each Feature on the Target (Using the Transformed Model)

intercept: coefficient = 5.24, $p < 2 \times 10^{-16}$; interpretation: predicted exam score when all predictors are set to zero.

Note: all covariates are significant in the final transformed model, meaning that each covariate has a significant association with final exam score when controlling for all other covariates. In other words, since the p-value for each covariate is less than the alpha = 0.05 significance level, the null hypothesis that each respective coefficient is zero is rejected, in favor of the alternative hypothesis that each respective coefficient non-zero.

study_hours_per day: coefficient = 9.59, $p < 2 \times 10^{-16}$; interpretation: Given two observations whose study hours per day values differ by 1 unit (hour), the observation with the larger study hours per day value will have an expected final exam score 9.59 units higher than the observation with the smaller study hours per day value.

log(mental_health_rating): coefficient = 7.56, $p < 2 \times 10^{-16}$; interpretation: Given two observations whose mental health rating values differ by 1%, the observation with the larger mental health rating value will have an expected final exam score 0.075 units (log(1.01)*7.56 units) higher than the observation with the smaller mental health rating value.

social_media_hours: coefficient = -2.65, $p < 2 \times 10^{-16}$; interpretation: Given two observations whose social media hours values differ by 1 unit (hour), the observation with the larger social media hours value will have an expected final exam score 2.65 units lower than the observation with the smaller social media hours value.

exercise_frequency: coefficient = 1.44, $p < 2 \times 10^{-16}$; interpretation: Given two observations whose exercise frequency values differ by 1 unit, the observation with the larger exercise frequency value will have an expected final exam score 1.44 units higher than the observation with the smaller exercise frequency value.

sleep_hours: coefficient = 2.05, $p < 2 \times 10^{-16}$; interpretation: Given two observations whose sleep hours per day values differ by 1 unit (hour), the observation with the larger sleep hours value will have an expected final exam score 2.05 units higher than the observation with the smaller sleep hours value.

netflix_hours: coefficient = -2.27, $p < 2 \times 10^{-16}$; interpretation: Given two observations whose Netflix hours values differ by 1 unit (hour), the observation with the larger Netflix hours value will have an expected final exam score -2.27 units lower than the observation with the smaller Netflix hours value.

attendance_percentage: coefficient = 0.14, $p < 8.81 \times 10^{-14}$; interpretation: Given two observations whose attendance percentage values differ by 1 unit (percent), the observation with the larger attendance percentage value will have an expected final exam score 0.14 units higher than the observation with the smaller attendance percentage value.


### Providing Confidence Intervals for Significant Coefficients
```{r}
# All coefficients of the model are significant
confint(log_final_mod, level = .95)
```
# Discussion

This statistical analysis aimed to identify key demographic, lifestyle, environmental, and academic factors that predict final exam performance among students. Using multiple linear regression and cross-validation on a dataset with a sample size of 1,000 students, both the final untransformed model and transformed model achieved high predictive accuracy, with adjusted $R^2$ valyes of 0.90 and 0.89, respectively. This means the models explain approximately 90% and 89% of the variance in final exam scores, respectively. All predictors in the final transformed model were statistically significant. Model assumption validation assessment confirmed that the assumptions of linear regression were largely met for the initial untransformed model, with no significant evidence of heteroscedasticity or serious violations of linearity, independence, or multicollinearity. The residuals were not normally distributed; however, leading to the transformed model, which satisfied all assumptions of linear regression but had worse predictive performance. This reflects a bias–variance tradeoff: the transformed model may have lower bias, but increased variance (worse prediction). For the purpose of interpreting coefficients, hypothesis testing, statistical inference, and reporting valid p-values and confidence intervals, the transformed model is preferred since it meets all linear regression assumptions, even if it is slightly worse in predictive performance. However, the untransformed model remains valuable for predictive tasks, despite violating the normality assumption. The original, untransformed model is valuable for its better predictive performance, despite it violating the normality of residuals assumption. Both the untransformed model and the transformed model produced similar coefficient estimates for most predictors. The primary difference was observed in the transformed covariate, which showed altered effect size due to the log transformation. This suggests that the main findings are robust to model specification. Overall, providing both models allows for both inference and predictive insight, while acknowledging the strengths and limitations of each model. Since my primary goal is to understand how demographic, lifestyle, academic, and environmental factors jointly relate to final exam scores among students (vs prediction), the transformed model is more ideal to use for my interpretations.

Looking at the summary results from the transformed model, Unsurprisingly, the number of hours a student studies per day has the highest positive impact on exam score. Hours spent away from studying doing more leisurely activities such as watching Netflix or using social media had significant negative impact on exam score. Lifestyle factors such as sleep, exercise, and mental health also are meaningfully associated with improved scores, suggesting it is not only studying that affects exam performance. This highlights the importance of overall well-being on academic performance. Interestingly, attendance percentage had only a modest, (although significant) impact. This brings questions as to how students make up missed learning on days they do not attend classes, possibly through self-study or accessing class materials in other ways.

# Conclusion

The findings emphasize that academic performance is influenced by a range of factors, not only study habits. While studying more remains the most impactful strategy, lifestyle factors such as sleep, mental health, and exercise also play critical roles. In contrast, limiting distractions such as excessive media use may help improve outcomes. These insights can inform both students and educators in designing strategies to support academic performance through both behavioral and environmental interventions. Further research should be more transparent about data collection methods, assess other factors such as self-perceived test-taking ability, and consider longitudinal designs to better understand how these predictors influence academic performance over time.
